from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import os

# ====== ì„¤ì • ======
BASE_URL = "https://www.mois.go.kr"
LIST_URL = "https://www.mois.go.kr/frt/bbs/type001/commonSelectBoardList.do?bbsId=BBSMSTR_000000000336"
DOWNLOAD_DIR = "/tmp"
TEXT_OUTPUT_PATH = "/tmp/latest_report.txt"

SLACK_WEBHOOK_URL = ""   # ðŸ‘‰ ì—¬ê¸°ì— ë³¸ì¸ Webhook URL ìž…ë ¥

# ====== DAG ê¸°ë³¸ ì„¤ì • ======
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2025, 11, 17),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    dag_id='mois_report_slack',
    default_args=default_args,
    schedule='0 7 * * *',   # ë§¤ì¼ ì˜¤í›„ 18ì‹œ 1ë¶„
    catchup=False
)

# ==================================
# 1) ìµœì‹  ë³´ê³ ì„œ ë§í¬ ìˆ˜ì§‘
# ==================================
def crawl_latest_report(**kwargs):
    response = requests.get(LIST_URL)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "html.parser")

    first_row = soup.select_one("table tbody tr:nth-of-type(1) a")
    if not first_row:
        raise Exception("ê²Œì‹œê¸€ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤!")

    file_name = soup.find('div', class_='wrap').text.strip()
    detail_relative_url = first_row["href"]

    detail_url = BASE_URL + detail_relative_url
    print("ìµœì‹  ë³´ê³ ì„œ URL :",detail_url)
    print("ë³´ê³ ì„œ :",file_name)

    kwargs['ti'].xcom_push(key='detail_url', value=detail_url)
    kwargs['ti'].xcom_push(key='file_name', value=file_name)

# ==================================
# 2) PDF ë‹¤ìš´ë¡œë“œ
# ==================================
def download_pdf(**kwargs):
    ti = kwargs['ti']
    detail_url = ti.xcom_pull(key='detail_url', task_ids='crawl_latest_report')
    file_name = ti.xcom_pull(key='file_name', task_ids='crawl_latest_report')

    response = requests.get(detail_url)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, 'html.parser')

    file_list_div = soup.find('div', class_='fileList')
    download_link_tag = file_list_div.find('a')

    relative_path = download_link_tag['href']
    full_download_url = urljoin(BASE_URL, relative_path)
    modified_download_url = full_download_url.replace("fileSn=0", "fileSn=1")

    download_path = os.path.join(DOWNLOAD_DIR, file_name)

    with requests.get(modified_download_url, stream=True) as r:
        r.raise_for_status()
        with open(download_path, 'wb') as f:
            for chunk in r.iter_content(chunk_size=1024):
                if chunk:
                    f.write(chunk)
    print("ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
    ti.xcom_push(key='download_path', value=download_path)

# ==================================
# 3) PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
# ==================================
def extract_pdf_text(**kwargs):
    import PyPDF2

    ti = kwargs['ti']
    download_path = ti.xcom_pull(key='download_path', task_ids='download_pdf')

    text = ""
    with open(download_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()

    os.remove(download_path)

    # í…ìŠ¤íŠ¸ íŒŒì¼ ì €ìž¥
    with open(TEXT_OUTPUT_PATH, "w", encoding="utf-8") as f:
        f.write(text)
    #print("í…ìŠ¤íŠ¸ ì¶”ì¶œ  ì™„ë£Œ\n", text[0:500])
    print("ë³´ê³ ì„œ ì—…ë¡œë“œ ê¸°ì¤€ :", text[34:52])
    print("ê¸°ìƒ í˜„í™© ë° ì „ë§ :", text[66:1000])
    send_text = "ë³´ê³ ì„œ ìž‘ì„±ì¼ :" + text[34:52] + '\n' + text
    #print("ë³´ê³ ì„œ :",send_text[0:1000])
    ti.xcom_push(key='raw_text', value=text)

# ==================================
# 4) LangChain ê¸°ë°˜ AI ìš”ì•½ ìƒì„±
# ==================================
def run_ai_agent(**kwargs):
    import requests
    from gradio_client import Client
    client = Client("amd/gpt-oss-120b-chatbot")
    ti = kwargs['ti']
    text = ti.xcom_pull(task_ids="extract_pdf_text", key="raw_text")
    query = f"""ë‹¤ìŒì€ ì¼ì¼ìƒí™©ë³´ê³ ì„œ ë‚´ìš©ì´ë‹¤.
                {text}
                
                ìœ„ ë‚´ìš©ì—ì„œ ê¸°ìƒ í˜„í™©ê³¼ ê¸°ìƒ ì „ë§ì— ëŒ€í•´ ë¶ˆë › í˜•íƒœë¡œ ì•Œë ¤ì¤˜"""
    output = client.predict(query)
    ti.xcom_push(key="ai_summary", value=output)


# ==================================
# 5) Slack ë©”ì‹œì§€ ë³´ë‚´ê¸°
# ==================================
def send_slack(**kwargs):
    import requests

    ti = kwargs['ti']
    summary = ti.xcom_pull(task_ids='run_ai_agent', key='ai_summary')

    payload = {
        "text": f"ðŸ“Œ *ì˜¤ëŠ˜ì˜ ì•ˆì „ê´€ë¦¬ìƒí™© ìš”ì•½*\n```{summary}```"
    }

    requests.post(SLACK_WEBHOOK_URL, json=payload)

# ==================================
# Operators
# ==================================
t1 = PythonOperator(task_id='crawl_latest_report', python_callable=crawl_latest_report, dag=dag)
t2 = PythonOperator(task_id='download_pdf', python_callable=download_pdf, dag=dag)
t3 = PythonOperator(task_id='extract_pdf_text', python_callable=extract_pdf_text, dag=dag)
t4 = PythonOperator(task_id='run_ai_agent', python_callable=run_ai_agent, dag=dag)
t5 = PythonOperator(task_id='send_slack', python_callable=send_slack, dag=dag)

# Task íë¦„
t1 >> t2 >> t3 >> t4 >> t5
